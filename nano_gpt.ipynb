{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProKelly/nanogpt/blob/master/nano_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVMLPrT7_fKz",
        "outputId": "a9176136-09f9-47e6-e4d5-a3d1ad3d8396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-15 16:06:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-03-15 16:06:12 (14.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DM5VAxHO_8V0"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m0bAQ7IAFql",
        "outputId": "a1c1159b-35bd-4ab6-de74-e440c80040e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK3IBwwLALre",
        "outputId": "a52f495c-a23f-4442-f770-d8c6943fa378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4eBbZHBAXAZ",
        "outputId": "3ee85e98-3946-470b-cbb2-3a89436038f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnt5mdYfAsZV",
        "outputId": "c3bae7cf-087d-4682-ceb6-1d63ca71b9d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJgNb43XCZlt",
        "outputId": "3c6d6810-913a-419a-d681-639f60371e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCydCbmTCr01"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_gmQaeiD_ih"
      },
      "source": [
        "we train our transformers on little chunks of the dataset so that the transfomer becomes useful with the context and will be able to use context to predict the next character and also because it's computationally more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5faA8dBdC0zS",
        "outputId": "9d2ba7cb-0d73-4793-8df1-b8a06e820525"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAWAtFu1D28z",
        "outputId": "ea0740f1-85d7-4340-fc2a-cc36141168b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo1bayfDEUlh",
        "outputId": "a7256cf1-d8cd-4403-9ebc-f170a52e617c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AXOln4UFNil"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1  # We generate one sequence at a time for conversational interaction\n",
        "block_size = 128  # Context length (in words, not characters)\n",
        "max_iters = 10000  # Training iterations\n",
        "eval_interval = 500  # Evaluation frequency\n",
        "learning_rate = 1e-3  # Adjusted learning rate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128  # Embedding size\n",
        "n_head = 8  # Number of attention heads\n",
        "n_layer = 6  # Number of transformer layers\n",
        "dropout = 0.1  # Regularization to prevent overfitting\n",
        "temperature = 0.7  # Temperature to adjust randomness of predictions\n",
        "max_grad_norm = 1.0  # Gradient clipping max norm\n",
        "patience = 5  # Early stopping patience (number of iterations without improvement)\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Load the Holy Bible dataset (or any other dataset you're using)\n",
        "bible_url = \"https://www.gutenberg.org/files/10/10-0.txt\"\n",
        "response = requests.get(bible_url)\n",
        "with open('bible.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "with open('bible.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenization (Word-Level instead of Characters)\n",
        "words = text.split()\n",
        "vocab = sorted(set(words))\n",
        "vocab_size = len(vocab)\n",
        "stoi = {word: i for i, word in enumerate(vocab)}  # Word-to-index mapping\n",
        "itos = {i: word for i, word in enumerate(vocab)}  # Index-to-word mapping\n",
        "\n",
        "encode = lambda s: [stoi[word] for word in s.split() if word in stoi]  # Convert text to token IDs\n",
        "decode = lambda l: ' '.join([itos[i] for i in l])  # Convert token IDs back to text\n",
        "\n",
        "# Convert dataset to tensors\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data batching\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "        self.layers = nn.ModuleList([nn.TransformerEncoderLayer(n_embd, n_head, 4 * n_embd, dropout) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits = logits.view(-1, vocab_size)\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=100, temperature=1.0):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]  # Only use the last `block_size` tokens for context\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature  # Scale by temperature\n",
        "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample next token\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Add new token to context\n",
        "        return idx\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer().to(device)\n",
        "print(f\"Model has {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # Weight decay for regularization\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)  # Reduce LR every 1000 iterations\n",
        "\n",
        "# Early Stopping variables\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "\n",
        "# Training loop with early stopping, gradient clipping, and LR scheduler\n",
        "for iter in range(max_iters):\n",
        "    model.train()\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # Early Stopping logic\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            early_stopping_counter = 0\n",
        "            # Save the model with the best validation loss\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(f\"Early stopping at step {iter} due to no improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "    # Data batch retrieval\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # Zero out gradients, backward pass, optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients to prevent exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # Step the scheduler to adjust learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "# Interactive chat loop\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Starting with an empty context\n",
        "print(\"Start chatting with the model! Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    # Take input from the user\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Encode the input and add it to the context\n",
        "    user_input_tokens = encode(user_input)\n",
        "    context = torch.cat((context, torch.tensor(user_input_tokens, dtype=torch.long, device=device).unsqueeze(0)), dim=1)\n",
        "\n",
        "    # Generate a response based on the current context\n",
        "    generated_tokens = model.generate(context, max_new_tokens=150, temperature=temperature)\n",
        "    response = decode(generated_tokens[0].tolist())\n",
        "\n",
        "    # Extract the generated response and display it\n",
        "    print(f\"Model: {response[len(user_input):].strip()}\")  # Remove user input part from the response\n"
      ],
      "metadata": {
        "id": "WcDjM9CWP728"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D24LSYdK4tfv"
      },
      "source": [
        "**Mini GPT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DBfggZN14vEF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1  # We generate one sequence at a time for conversational interaction\n",
        "block_size = 128  # Context length (in words, not characters)\n",
        "max_iters = 10000  # Training iterations\n",
        "eval_interval = 500  # Evaluation frequency\n",
        "learning_rate = 1e-3  # Adjusted learning rate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128  # Embedding size\n",
        "n_head = 8  # Number of attention heads\n",
        "n_layer = 6  # Number of transformer layers\n",
        "dropout = 0.1  # Regularization to prevent overfitting\n",
        "temperature = 0.7  # Temperature to adjust randomness of predictions\n",
        "max_grad_norm = 1.0  # Gradient clipping max norm\n",
        "patience = 5  # Early stopping patience (number of iterations without improvement)\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Load the Holy Bible dataset (or any other dataset you're using)\n",
        "bible_url = \"https://www.gutenberg.org/files/10/10-0.txt\"\n",
        "response = requests.get(bible_url)\n",
        "with open('bible.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "with open('bible.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word level Tokenization"
      ],
      "metadata": {
        "id": "lrjvF51AZ5bp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tSt4d7tO6tu0"
      },
      "outputs": [],
      "source": [
        "# Tokenization (Word-Level instead of Characters)\n",
        "words = text.split()\n",
        "vocab = sorted(set(words))\n",
        "vocab_size = len(vocab)\n",
        "stoi = {word: i for i, word in enumerate(vocab)}  # Word-to-index mapping\n",
        "itos = {i: word for i, word in enumerate(vocab)}  # Index-to-word mapping\n",
        "\n",
        "encode = lambda s: [stoi[word] for word in s.split() if word in stoi]  # Convert text to token IDs\n",
        "decode = lambda l: ' '.join([itos[i] for i in l])  # Convert token IDs back to text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batching"
      ],
      "metadata": {
        "id": "dVeiFXdZaHYl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A2NJo_8b4zwK"
      },
      "outputs": [],
      "source": [
        "# Convert dataset to tensors\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data batching\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Model"
      ],
      "metadata": {
        "id": "xLv3y2DLaVaW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-6ZuFS9R6ZwJ"
      },
      "outputs": [],
      "source": [
        "# Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "        self.layers = nn.ModuleList([nn.TransformerEncoderLayer(n_embd, n_head, 4 * n_embd, dropout) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits = logits.view(-1, vocab_size)\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=100, temperature=1.0):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]  # Only use the last `block_size` tokens for context\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature  # Scale by temperature\n",
        "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample next token\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Add new token to context\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the Model"
      ],
      "metadata": {
        "id": "IavXd3g2afEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = Transformer().to(device)\n",
        "print(f\"Model has {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # Weight decay for regularization\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)  # Reduce LR every 1000 iterations\n",
        "\n",
        "# Early Stopping variables\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAzHI9mZad0N",
        "outputId": "ea4f638b-3767-4579-ffda-376ad5dd80d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 9.80M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "YH0m1vvKaib7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeUgbFBS6bXF",
        "outputId": "bf31d851-e2e7-4eeb-ca03-4278d63c6d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Train loss 10.5402, Val loss 10.5469\n",
            "Step 500: Train loss 6.6828, Val loss 7.2006\n",
            "Step 1000: Train loss 6.5243, Val loss 7.1707\n",
            "Step 1500: Train loss 6.5019, Val loss 7.1607\n",
            "Step 2000: Train loss 6.4797, Val loss 7.0734\n",
            "Step 2500: Train loss 6.4552, Val loss 7.0031\n",
            "Step 3000: Train loss 6.4747, Val loss 7.0967\n",
            "Step 3500: Train loss 6.3755, Val loss 7.0651\n",
            "Step 4000: Train loss 6.4648, Val loss 7.1876\n",
            "Step 4500: Train loss 6.3441, Val loss 7.0118\n",
            "Step 5000: Train loss 6.3844, Val loss 7.0466\n",
            "Early stopping at step 5000 due to no improvement in validation loss.\n"
          ]
        }
      ],
      "source": [
        "# Training loop with early stopping, gradient clipping, and LR scheduler\n",
        "for iter in range(max_iters):\n",
        "    model.train()\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # Early Stopping logic\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            early_stopping_counter = 0\n",
        "            # Save the model with the best validation loss\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(f\"Early stopping at step {iter} due to no improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "    # Data batch retrieval\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # Zero out gradients, backward pass, optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients to prevent exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # Step the scheduler to adjust learning rate\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with the Model"
      ],
      "metadata": {
        "id": "tBgNWBJ9bK1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive chat loop\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Starting with an empty context\n",
        "print(\"Start chatting with the model! Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    # Take input from the user\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Encode the input and add it to the context\n",
        "    user_input_tokens = encode(user_input)\n",
        "    context = torch.cat((context, torch.tensor(user_input_tokens, dtype=torch.long, device=device).unsqueeze(0)), dim=1)\n",
        "\n",
        "    # Generate a response based on the current context\n",
        "    generated_tokens = model.generate(context, max_new_tokens=150, temperature=temperature)\n",
        "    response = decode(generated_tokens[0].tolist())\n",
        "\n",
        "    # Extract the generated response and display it\n",
        "    print(f\"Model: {response[len(user_input):].strip()}\")  # Remove user input part from the response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V653aS6GbD8m",
        "outputId": "a5860665-114e-45cb-8126-aaaaef9750bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start chatting with the model! Type 'exit' to quit.\n",
            "\n",
            "Model: ho is Jesus and a hand And in ye and the sea a house and the land a LORD. and smite she and the doors the people of the LORD; is Jesus the crown a saints: And the king which a righteous, for the LORD hast ye But my have and I it But the LORD, and thou shall unto the multitude of the son shall shall and they of the people and the married and the house of him. shall of the people And the tongue the land will the bed and by Israel, And cast And the LORD, and upon him of the unclean and he of his people and giveth the LORD for a sabbath to the top unto man and shall that it but it of her unto his LORD, in the ark, is to the terrible and be is to the fire I that it thither. with the\n",
            "Model: ding who is Jesus Heaven of the south. and of the wall of the children among the heaven of Israel, of the land And was as to the blood and my shall the midst And command with thy of the land thy shekels and I of his land the border and the word with the little gather shall the LORD; And he man that said, of the glorious in the man them have and they of the firstfruits, him, And the mouth and he up and of war, and for the LORD, will in thee. of the hands of his name his other of the rest of the noise that God, from the princes in the people shall a keeper and it as the king thou God to the men for the LORD and thy And I unto whom he of the tabernacle of the people and he and he but he the LORD even\n",
            "Model: rding who is Jesus Heaven thee in the temple, of their wreathen and which made let the son and his good in the golden to the land and ye and from the LORD he and with the LORD to the sweat and the way, with the children and the men of the LORD, to the son of the other unto the LORD shalt in the end, of the priests and she and he do Moses to the voice a heavens and the priest unto the house and was the god, in thee and they of the land And the certain of the LORD and the LORD and which the firstborn, fight to the service and are and he and of the LORD, and the service of thy shall is the hundred and the man of the voice was be And the thou, And go to their bones and are when he which to the people,\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOlJqq2ksh2Encgn8koFTIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}